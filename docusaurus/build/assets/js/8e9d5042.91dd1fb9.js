"use strict";(self.webpackChunkdeep_dive_yolo=self.webpackChunkdeep_dive_yolo||[]).push([[12],{5156:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"yolov5","title":"The YOLOv5 Playbook","description":"A Deep Dive into Object Detection","source":"@site/docs/yolov5.md","sourceDirName":".","slug":"/yolov5","permalink":"/yolov5","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Welcome to deep-dive-yolo","permalink":"/"}}');var s=r(4848),t=r(8453);const o={},l="The YOLOv5 Playbook",a={},d=[{value:"A Deep Dive into Object Detection",id:"a-deep-dive-into-object-detection",level:2},{value:"Authors : Huangyi Qin, ChatGPction",id:"authors--huangyi-qin-chatgpction",level:4},{value:"Introduction",id:"introduction",level:2},{value:"What is YOLOv5?",id:"what-is-yolov5",level:3},{value:"\ud83d\udd39 Key Features of YOLOv5:",id:"-key-features-of-yolov5",level:4},{value:"Evolution of YOLO: From YOLOv1 to YOLOv5",id:"evolution-of-yolo-from-yolov1-to-yolov5",level:3},{value:"Why YOLOv5? Use Cases &amp; Advantages",id:"why-yolov5-use-cases--advantages",level:3},{value:"\ud83d\udd39 <strong>Use Cases of YOLOv5</strong>",id:"-use-cases-of-yolov5",level:4},{value:"\ud83d\ude80 <strong>Why Choose YOLOv5 Over Other Models?</strong>",id:"-why-choose-yolov5-over-other-models",level:4},{value:"Installation &amp; Environment Setup",id:"installation--environment-setup",level:4},{value:"\ud83d\udd39 Step 1: Install Ultralytics",id:"-step-1-install-ultralytics",level:5},{value:"\ud83d\udd39 Step 2: Verify Installation",id:"-step-2-verify-installation",level:5},{value:"\ud83d\udd39 Step 3: Running YOLOv5 CLI",id:"-step-3-running-yolov5-cli",level:5},{value:"\ud83d\udd39 Step 4: Training a Custom Model",id:"-step-4-training-a-custom-model",level:5},{value:"<strong>Inference &amp; Basic examples</strong>",id:"inference--basic-examples",level:2},{value:"<strong>Running Inference on Images &amp; Videos with Ultralytics YOLOv5</strong>",id:"running-inference-on-images--videos-with-ultralytics-yolov5",level:3},{value:"<strong>1. Installing the Required Dependencies</strong>",id:"1-installing-the-required-dependencies",level:4},{value:"<strong>2. Importing the YOLOv5 Model</strong>",id:"2-importing-the-yolov5-model",level:4},{value:"<strong>3. Running Inference on an Image</strong>",id:"3-running-inference-on-an-image",level:4},{value:"<strong>4. Running Inference on a Video</strong>",id:"4-running-inference-on-a-video",level:4},{value:"<strong>5. Running Inference on a Folder of Images</strong>",id:"5-running-inference-on-a-folder-of-images",level:4},{value:"<strong>6. Saving Results to a Folder</strong>",id:"6-saving-results-to-a-folder",level:4},{value:"<strong>7. Running Inference with OpenCV (Real-Time Detection)</strong>",id:"7-running-inference-with-opencv-real-time-detection",level:4},{value:"<strong>8. Running YOLOv5 on an Online Image</strong>",id:"8-running-yolov5-on-an-online-image",level:4},{value:"<strong>9. Running Inference on GPU (CUDA)</strong>",id:"9-running-inference-on-gpu-cuda",level:4},{value:"<strong>10. Saving Inference Results as JSON</strong>",id:"10-saving-inference-results-as-json",level:4},{value:"<strong>11. Extracting Detected Objects and Bounding Boxes</strong>",id:"11-extracting-detected-objects-and-bounding-boxes",level:4},{value:"<strong>12. Running Inference on Bayer Images with YOLOv5</strong>",id:"12-running-inference-on-bayer-images-with-yolov5",level:4},{value:"<strong>Conclusion</strong>",id:"conclusion",level:3},{value:"Optimizing YOLOv5 with TensorRT",id:"optimizing-yolov5-with-tensorrt",level:2},{value:"Introduction",id:"introduction-1",level:3},{value:"Todo optimizing",id:"todo-optimizing",level:3},{value:"Install Dependencies",id:"install-dependencies",level:4},{value:"Step 1: Export YOLOv5 to TensorRT Engine",id:"step-1-export-yolov5-to-tensorrt-engine",level:4},{value:"Step 2: Run Inference with TensorRT",id:"step-2-run-inference-with-tensorrt",level:4},{value:"Step 3: Benchmark Performance",id:"step-3-benchmark-performance",level:4},{value:"Conclusion",id:"conclusion-1",level:3},{value:"Additional To-Do: Debayer with TensorRT",id:"additional-to-do-debayer-with-tensorrt",level:3},{value:"Overview",id:"overview",level:4},{value:"<strong>Handling Large Images</strong>",id:"handling-large-images",level:2},{value:"<strong>Apply Sliding Window</strong>",id:"apply-sliding-window",level:3},{value:"<strong>Implementation: Sliding Window Extraction</strong>",id:"implementation-sliding-window-extraction",level:4},{value:"<strong>Merge Sliding Window and YOLOv5 results</strong>",id:"merge-sliding-window-and-yolov5-results",level:3},{value:"<strong>Applying YOLOv5 to Patches</strong>",id:"applying-yolov5-to-patches",level:4},{value:"<strong>Conclusion</strong>",id:"conclusion-2",level:3},{value:"Multi-Camera &amp; Large-Scale Image Processing",id:"multi-camera--large-scale-image-processing",level:2},{value:"Deploying on Edge Devices (Raspberry Pi, Jetson Nano)",id:"deploying-on-edge-devices-raspberry-pi-jetson-nano",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"the-yolov5-playbook",children:"The YOLOv5 Playbook"})}),"\n",(0,s.jsx)(n.h2,{id:"a-deep-dive-into-object-detection",children:"A Deep Dive into Object Detection"}),"\n",(0,s.jsx)(n.h4,{id:"authors--huangyi-qin-chatgpction",children:"Authors : Huangyi Qin, ChatGPction"}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.h3,{id:"what-is-yolov5",children:"What is YOLOv5?"}),"\n",(0,s.jsx)(n.p,{children:"YOLOv5 (You Only Look Once v5) is a state-of-the-art real-time object detection model developed by Ultralytics. It is based on a single-stage convolutional neural network (CNN) and is designed for high-speed detection with impressive accuracy. YOLOv5 builds upon the strengths of its predecessors while incorporating improvements in efficiency, model scaling, and training stability."}),"\n",(0,s.jsx)(n.h4,{id:"-key-features-of-yolov5",children:"\ud83d\udd39 Key Features of YOLOv5:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Fast & Lightweight"}),": Optimized for real-time performance, even on edge devices."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High Accuracy"}),": Uses advanced anchor box mechanisms and improved loss functions."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Modular & Flexible"}),": Supports multiple sizes (YOLOv5s, YOLOv5m, YOLOv5l, YOLOv5x)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Built-in Augmentations"}),": Leverages techniques like Mosaic Augmentation for better training."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Easy Deployment"}),": Compatible with PyTorch, TensorRT, and ONNX for seamless integration."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"evolution-of-yolo-from-yolov1-to-yolov5",children:"Evolution of YOLO: From YOLOv1 to YOLOv5"}),"\n",(0,s.jsx)(n.p,{children:"YOLO (You Only Look Once) was introduced by Joseph Redmon in 2016 and has undergone several iterations:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Version"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Year"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Key Improvements"})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"YOLOv1"})}),(0,s.jsx)(n.td,{children:"2016"}),(0,s.jsx)(n.td,{children:"First real-time object detector using a single CNN."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"YOLOv2"})}),(0,s.jsx)(n.td,{children:"2017"}),(0,s.jsx)(n.td,{children:"Introduced batch normalization, anchor boxes, and higher accuracy."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"YOLOv3"})}),(0,s.jsx)(n.td,{children:"2018"}),(0,s.jsx)(n.td,{children:"Multi-scale detection, Darknet-53 backbone, better small object detection."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"YOLOv4"})}),(0,s.jsx)(n.td,{children:"2020"}),(0,s.jsx)(n.td,{children:"Optimized CSPDarknet53, SPP, and Mish activation."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"YOLOv5"})}),(0,s.jsx)(n.td,{children:"2020"}),(0,s.jsx)(n.td,{children:"PyTorch-based, lighter, faster, and improved augmentation techniques."})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:["YOLOv5 is ",(0,s.jsx)(n.strong,{children:"not an official continuation"})," of YOLOv4 by the original creators, but rather a PyTorch-based implementation by ",(0,s.jsx)(n.strong,{children:"Ultralytics"})," with significant performance optimizations."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"why-yolov5-use-cases--advantages",children:"Why YOLOv5? Use Cases & Advantages"}),"\n",(0,s.jsx)(n.p,{children:"YOLOv5 is widely adopted in various real-world applications due to its speed and efficiency:"}),"\n",(0,s.jsxs)(n.h4,{id:"-use-cases-of-yolov5",children:["\ud83d\udd39 ",(0,s.jsx)(n.strong,{children:"Use Cases of YOLOv5"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Autonomous Vehicles"})," \ud83d\ude97 \u2192 Object detection for pedestrian & vehicle tracking."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Surveillance & Security"})," \ud83d\udcf9 \u2192 Face & anomaly detection in real-time."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Medical Imaging"})," \ud83c\udfe5 \u2192 Tumor and X-ray image analysis."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Retail & Inventory"})," \ud83d\udecd\ufe0f \u2192 Automated checkout systems, shelf monitoring."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Industrial Automation"})," \ud83c\udfed \u2192 Defect detection in manufacturing."]}),"\n"]}),"\n",(0,s.jsxs)(n.h4,{id:"-why-choose-yolov5-over-other-models",children:["\ud83d\ude80 ",(0,s.jsx)(n.strong,{children:"Why Choose YOLOv5 Over Other Models?"})]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Feature"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"YOLOv5"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"Faster R-CNN"})}),(0,s.jsx)(n.th,{children:(0,s.jsx)(n.strong,{children:"SSD"})})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Speed (FPS)"})}),(0,s.jsxs)(n.td,{children:["\ud83d\ude80 ",(0,s.jsx)(n.strong,{children:"Fastest"})," (~100 FPS on RTX 3090)"]}),(0,s.jsx)(n.td,{children:"Slow (~5 FPS)"}),(0,s.jsx)(n.td,{children:"Medium (~20 FPS)"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Accuracy"})}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Very High"}),(0,s.jsx)(n.td,{children:"Moderate"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Hardware Friendly"})}),(0,s.jsx)(n.td,{children:"\u2705 Yes"}),(0,s.jsx)(n.td,{children:"\u274c No (High Compute)"}),(0,s.jsx)(n.td,{children:"\u2705 Yes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Real-time Capability"})}),(0,s.jsx)(n.td,{children:"\u2705 Yes"}),(0,s.jsx)(n.td,{children:"\u274c No"}),(0,s.jsx)(n.td,{children:"\u2705 Yes"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"installation--environment-setup",children:"Installation & Environment Setup"}),"\n",(0,s.jsxs)(n.p,{children:["To get started with YOLOv5, install the necessary dependencies and clone the official repository.\r\nThe easiest way to use ",(0,s.jsx)(n.strong,{children:"YOLOv5"})," is through the ",(0,s.jsx)(n.strong,{children:"Ultralytics package"}),", which simplifies training, inference, and deployment."]}),"\n",(0,s.jsx)(n.h5,{id:"-step-1-install-ultralytics",children:"\ud83d\udd39 Step 1: Install Ultralytics"}),"\n",(0,s.jsxs)(n.p,{children:["Install the package directly from ",(0,s.jsx)(n.strong,{children:"pip"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install ultralytics opencv-python pillow matplotlib\n"})}),"\n",(0,s.jsx)(n.p,{children:"This automatically installs YOLOv5 and all necessary dependencies."}),"\n",(0,s.jsx)(n.h5,{id:"-step-2-verify-installation",children:"\ud83d\udd39 Step 2: Verify Installation"}),"\n",(0,s.jsx)(n.p,{children:"Check if YOLOv5 is installed correctly:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\r\n\r\n# Load a pre-trained model\r\nmodel = YOLO('yolov5s.pt')\r\n\r\n# Run inference on an image\r\nresults = model('https://ultralytics.com/images/zidane.jpg')\r\n\r\n# Display results\r\nresults.show()\n"})}),"\n",(0,s.jsx)(n.h5,{id:"-step-3-running-yolov5-cli",children:"\ud83d\udd39 Step 3: Running YOLOv5 CLI"}),"\n",(0,s.jsx)(n.p,{children:"You can also use the YOLOv5 command-line interface (CLI) for inference:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"yolo task=detect mode=predict model=yolov5s.pt source=https://ultralytics.com/images/zidane.jpg\n"})}),"\n",(0,s.jsx)(n.h5,{id:"-step-4-training-a-custom-model",children:"\ud83d\udd39 Step 4: Training a Custom Model"}),"\n",(0,s.jsx)(n.p,{children:"Train YOLOv5 on your own dataset:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"yolo task=detect mode=train model=yolov5s.pt data=coco128.yaml epochs=50 imgsz=640\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"inference--basic-examples",children:(0,s.jsx)(n.strong,{children:"Inference & Basic examples"})}),"\n",(0,s.jsx)(n.h3,{id:"running-inference-on-images--videos-with-ultralytics-yolov5",children:(0,s.jsx)(n.strong,{children:"Running Inference on Images & Videos with Ultralytics YOLOv5"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Ultralytics YOLOv5"})," library provides a simple way to run inference on images and videos using Python. This section covers step-by-step instructions for setting up YOLOv5, running inference, and displaying the results using ",(0,s.jsx)(n.strong,{children:"Ultralytics Python API"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"1-installing-the-required-dependencies",children:(0,s.jsx)(n.strong,{children:"1. Installing the Required Dependencies"})}),"\n",(0,s.jsx)(n.p,{children:"Before running inference, install the required dependencies:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install ultralytics opencv-python pillow matplotlib\n"})}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"ultralytics"})," package includes YOLOv5 along with the latest YOLOv8 versions."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"2-importing-the-yolov5-model",children:(0,s.jsx)(n.strong,{children:"2. Importing the YOLOv5 Model"})}),"\n",(0,s.jsxs)(n.p,{children:["To use YOLOv5 in Python, we need to import the ",(0,s.jsx)(n.code,{children:"YOLO"})," class from the ",(0,s.jsx)(n.code,{children:"ultralytics"})," library:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"3-running-inference-on-an-image",children:(0,s.jsx)(n.strong,{children:"3. Running Inference on an Image"})}),"\n",(0,s.jsx)(n.p,{children:"To detect objects in an image, use the following Python script:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'print(\'```\')\r\nfrom ultralytics import YOLO\r\nimport cv2\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\n\r\n# Load a pre-trained YOLOv5 model\r\nmodel = YOLO("yolov5s.pt")  \r\n\r\n# Run inference on an image\r\nresults = model("https://ultralytics.com/images/bus.jpg", conf=0.5)\r\n\r\n# Save and show results\r\nfor i, result in enumerate(results):\r\n    img = result.plot()  # Get image with detections\r\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\r\nprint(\'```\')\r\n\r\n# Display the image in the notebook\r\nplt.figure(figsize=(10, 6))\r\nplt.imshow(img)\r\nplt.axis("off")\r\nplt.show()\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"PRO TIP  Replace 'model=yolov5s.pt' with new 'model=yolov5su.pt'.\r\nYOLOv5 'u' models are trained with https://github.com/ultralytics/ultralytics and feature improved performance vs standard YOLOv5 models trained with https://github.com/ultralytics/yolov5.\r\n\r\n\r\nFound https://ultralytics.com/images/bus.jpg locally at bus.jpg\r\nimage 1/1 D:\\github\\qinhy\\deep-dive-yolo\\docusaurus\\src\\bus.jpg: 640x480 4 persons, 1 bus, 15.8ms\r\nSpeed: 0.0ms preprocess, 15.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 480)\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:r(4694).A+"",width:"366",height:"482"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The model loads ",(0,s.jsx)(n.strong,{children:"YOLOv5s"})," (the smallest version, best for fast inference)."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"model()"})," method performs inference with a confidence threshold of 0.5."]}),"\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.code,{children:"show()"})," method displays the image with bounding boxes and labels."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"4-running-inference-on-a-video",children:(0,s.jsx)(n.strong,{children:"4. Running Inference on a Video"})}),"\n",(0,s.jsx)(n.p,{children:"To process an entire video and display object detections:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\r\nfrom ultralytics import YOLO\r\n\r\n## Load YOLOv5 model\r\nmodel = YOLO("yolov5s.pt")\r\n\r\n## Open video file or webcam (use "0" for webcam)\r\ncap = cv2.VideoCapture("video.mp4")\r\n\r\nwhile cap.isOpened():\r\n    ret, frame = cap.read()\r\n    if not ret:\r\n        break\r\n\r\n    ## Run inference\r\n    results = model(frame, conf=0.5)\r\n\r\n    ## Render the results on the frame\r\n    for result in results:\r\n        result.show()  ## Shows detections on the frame\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reads frames from a video file or webcam."}),"\n",(0,s.jsx)(n.li,{children:"Runs inference on each frame."}),"\n",(0,s.jsx)(n.li,{children:"Displays real-time detections."}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"5-running-inference-on-a-folder-of-images",children:(0,s.jsx)(n.strong,{children:"5. Running Inference on a Folder of Images"})}),"\n",(0,s.jsx)(n.p,{children:"To process all images in a folder:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import glob\r\nfrom ultralytics import YOLO\r\nfrom PIL import Image\r\n\r\n## Load the model\r\nmodel = YOLO("yolov5s.pt")\r\n\r\n## Get all images from a folder\r\nimage_paths = glob.glob("path/to/images/*.jpg")\r\n\r\nfor img_path in image_paths:\r\n    results = model(img_path, conf=0.5)\r\n    for result in results:\r\n        result.show()  ## Displays detections\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"6-saving-results-to-a-folder",children:(0,s.jsx)(n.strong,{children:"6. Saving Results to a Folder"})}),"\n",(0,s.jsx)(n.p,{children:"To save output images with detections:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'results = model("image.jpg", save=True, project="output_folder")\n'})}),"\n",(0,s.jsxs)(n.p,{children:["This saves the detection results inside the ",(0,s.jsx)(n.strong,{children:"output_folder/runs/detect/exp/"})," directory."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"7-running-inference-with-opencv-real-time-detection",children:(0,s.jsx)(n.strong,{children:"7. Running Inference with OpenCV (Real-Time Detection)"})}),"\n",(0,s.jsx)(n.p,{children:"For real-time object detection with OpenCV:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\r\nfrom ultralytics import YOLO\r\n\r\n## Load YOLO model\r\nmodel = YOLO("yolov5s.pt")\r\n\r\n## Open webcam (0 = default webcam, change to video file path for video)\r\ncap = cv2.VideoCapture(0)\r\n\r\nwhile cap.isOpened():\r\n    ret, frame = cap.read()\r\n    if not ret:\r\n        break\r\n\r\n    ## Run YOLOv5 inference\r\n    results = model(frame, conf=0.5)\r\n\r\n    ## Render results on the frame\r\n    for result in results:\r\n        annotated_frame = result.plot()  ## Draw bounding boxes\r\n        cv2.imshow("YOLOv5 Real-Time Detection", annotated_frame)\r\n\r\n    if cv2.waitKey(1) & 0xFF == ord("q"):  ## Press \'q\' to exit\r\n        break\r\n\r\ncap.release()\r\ncv2.destroyAllWindows()\n'})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Uses OpenCV to capture video frames from a webcam."}),"\n",(0,s.jsx)(n.li,{children:"Runs YOLOv5 inference on each frame."}),"\n",(0,s.jsxs)(n.li,{children:["Draws bounding boxes using ",(0,s.jsx)(n.code,{children:"result.plot()"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Press ",(0,s.jsx)(n.strong,{children:"'q'"})," to stop the detection."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"8-running-yolov5-on-an-online-image",children:(0,s.jsx)(n.strong,{children:"8. Running YOLOv5 on an Online Image"})}),"\n",(0,s.jsx)(n.p,{children:"To run inference on an image from the internet:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import requests\r\nfrom PIL import Image\r\nfrom ultralytics import YOLO\r\n\r\n## Load YOLOv5 model\r\nmodel = YOLO("yolov5s.pt")\r\n\r\n## Download image\r\nimage_url = "https://ultralytics.com/images/zidane.jpg"\r\nimage = Image.open(requests.get(image_url, stream=True).raw)\r\n\r\n## Run inference\r\nresults = model(image, conf=0.5)\r\n\r\n## Show results\r\nfor result in results:\r\n    result.show()\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"9-running-inference-on-gpu-cuda",children:(0,s.jsx)(n.strong,{children:"9. Running Inference on GPU (CUDA)"})}),"\n",(0,s.jsxs)(n.p,{children:["If you have a ",(0,s.jsx)(n.strong,{children:"GPU"}),", you can speed up inference by specifying the device:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'model = YOLO("yolov5s.pt").to("cuda")  ## Use "cuda" for GPU, "cpu" for CPU\r\nresults = model("image.jpg")\n'})}),"\n",(0,s.jsx)(n.p,{children:"To check if CUDA is available:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import torch\r\nprint(torch.cuda.is_available())  ## True if GPU is available\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"10-saving-inference-results-as-json",children:(0,s.jsx)(n.strong,{children:"10. Saving Inference Results as JSON"})}),"\n",(0,s.jsxs)(n.p,{children:["To save detection results as ",(0,s.jsx)(n.strong,{children:"JSON"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'results = model("image.jpg", conf=0.5)\r\ndetections = results[0].tojson()\r\n\r\n## Save to a file\r\nwith open("detections.json", "w") as f:\r\n    f.write(detections)\n'})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"11-extracting-detected-objects-and-bounding-boxes",children:(0,s.jsx)(n.strong,{children:"11. Extracting Detected Objects and Bounding Boxes"})}),"\n",(0,s.jsx)(n.p,{children:"If you need structured detection results:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'results = model("image.jpg", conf=0.5)\r\nfor result in results:\r\n    for box in result.boxes:\r\n        print(f"Class: {result.names[int(box.cls)]}, Confidence: {box.conf.item()}, BBox: {box.xyxy.tolist()}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:["This prints ",(0,s.jsx)(n.strong,{children:"detected class, confidence score, and bounding box coordinates"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"12-running-inference-on-bayer-images-with-yolov5",children:(0,s.jsx)(n.strong,{children:"12. Running Inference on Bayer Images with YOLOv5"})}),"\n",(0,s.jsxs)(n.p,{children:["Bayer images (raw sensor images) contain ",(0,s.jsx)(n.strong,{children:"Bayer color filter arrays (CFA)"}),", where each pixel represents only ",(0,s.jsx)(n.strong,{children:"one color channel (R, G, or B)"})," instead of full RGB data. To use YOLOv5 for inference on ",(0,s.jsx)(n.strong,{children:"Bayer images"}),", we must first ",(0,s.jsx)(n.strong,{children:"demosaic (debayer)"})," the image to reconstruct the full RGB image before running inference."]}),"\n",(0,s.jsxs)(n.p,{children:["To process a Bayer image, we must first ",(0,s.jsx)(n.strong,{children:"convert it into an RGB image"}),". Below is a script to ",(0,s.jsx)(n.strong,{children:"read, debayer, and run YOLOv5"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\nimport rawpy\r\nfrom ultralytics import YOLO\r\n\r\n# Load YOLO model\r\nmodel = YOLO("yolov5s.pt")\r\n\r\n# Function to process Bayer image\r\ndef process_bayer_image(bayer_image_path):\r\n    # Open the raw Bayer image using RawPy\r\n    with rawpy.imread(bayer_image_path) as raw:\r\n        rgb_image = raw.postprocess()  # Demosaic (convert Bayer to RGB)\r\n    \r\n    return rgb_image\r\n\r\n# Load and convert Bayer image\r\nbayer_image_path = "image.dng"  # Change this to your Bayer image path\r\nrgb_image = process_bayer_image(bayer_image_path)\r\n\r\n# Convert RGB image from RawPy (uint16) to uint8 (0-255 range)\r\nrgb_image = cv2.normalize(rgb_image, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\r\n\r\n# Run inference\r\nresults = model(rgb_image, conf=0.5)\r\n\r\n# Show results\r\nfor result in results:\r\n    result.show()\n'})}),"\n",(0,s.jsxs)(n.p,{children:["If your Bayer image is stored as a ",(0,s.jsx)(n.strong,{children:"single-channel grayscale image"}),", such as a ",(0,s.jsx)(n.code,{children:".tiff"}),", ",(0,s.jsx)(n.code,{children:".pgm"}),", or ",(0,s.jsx)(n.code,{children:".raw"})," file, you can manually apply ",(0,s.jsx)(n.strong,{children:"demosaicing"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\r\nimport numpy as np\r\nfrom ultralytics import YOLO\r\n\r\n# Load YOLO model\r\nmodel = YOLO("yolov5s.pt")\r\n\r\n# Load Bayer image (grayscale single-channel)\r\nbayer_image = cv2.imread("bayer_image.raw", cv2.IMREAD_GRAYSCALE)\r\n\r\n# Convert Bayer image to RGB using OpenCV\'s demosaicing\r\nrgb_image = cv2.cvtColor(bayer_image, cv2.COLOR_BAYER_BG2RGB)  # Adjust Bayer pattern (BG, GB, RG, etc.)\r\n\r\n# Run YOLO inference\r\nresults = model(rgb_image, conf=0.5)\r\n\r\n# Show results\r\nfor result in results:\r\n    result.show()\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Different cameras use different ",(0,s.jsx)(n.strong,{children:"Bayer filter arrangements"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"BGGR"})," (",(0,s.jsx)(n.code,{children:"cv2.COLOR_BAYER_BG2RGB"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RGGB"})," (",(0,s.jsx)(n.code,{children:"cv2.COLOR_BAYER_RG2RGB"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GRBG"})," (",(0,s.jsx)(n.code,{children:"cv2.COLOR_BAYER_GR2RGB"}),")"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GBRG"})," (",(0,s.jsx)(n.code,{children:"cv2.COLOR_BAYER_GB2RGB"}),")"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Make sure to check your ",(0,s.jsx)(n.strong,{children:"camera sensor's Bayer pattern"})," and adjust accordingly."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"conclusion",children:(0,s.jsx)(n.strong,{children:"Conclusion"})}),"\n",(0,s.jsxs)(n.p,{children:["With ",(0,s.jsx)(n.strong,{children:"Ultralytics YOLOv5 Python API"}),", you can efficiently run object detection on images, videos, real-time streams, and folders. The library supports ",(0,s.jsx)(n.strong,{children:"GPU acceleration, JSON output, OpenCV integration"}),", and ",(0,s.jsx)(n.strong,{children:"batch processing"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["In the next section, we will explore ",(0,s.jsx)(n.strong,{children:"deployment strategies"}),", including ",(0,s.jsx)(n.strong,{children:"exporting YOLOv5 to ONNX and TensorRT for faster inference on edge devices"}),". \ud83d\ude80"]}),"\n",(0,s.jsx)(n.h2,{id:"optimizing-yolov5-with-tensorrt",children:"Optimizing YOLOv5 with TensorRT"}),"\n",(0,s.jsx)(n.h3,{id:"introduction-1",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"TensorRT is an optimization tool from NVIDIA designed to accelerate deep learning inference on GPUs. By leveraging TensorRT, YOLOv5 can run significantly faster with reduced latency, making it ideal for real-time applications. This guide provides a step-by-step approach to exporting and running YOLOv5 using TensorRT."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"todo-optimizing",children:"Todo optimizing"}),"\n",(0,s.jsx)(n.p,{children:"Ensure that you have the following installed:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"NVIDIA GPU"})," with CUDA support"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CUDA Toolkit"})," (compatible version)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"cuDNN"})," (matching your CUDA version)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TensorRT"})," (latest stable release)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"PyTorch"})," (for model handling)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Ultralytics YOLO library"})," (for exporting and inference)"]}),"\n"]}),"\n",(0,s.jsx)(n.h4,{id:"install-dependencies",children:"Install Dependencies"}),"\n",(0,s.jsx)(n.p,{children:"You can install the required dependencies using:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install torch torchvision torchaudio\r\npip install ultralytics\r\npip install onnx\r\npip install tensorrt\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"step-1-export-yolov5-to-tensorrt-engine",children:"Step 1: Export YOLOv5 to TensorRT Engine"}),"\n",(0,s.jsxs)(n.p,{children:["The following Python script exports a YOLOv5 model to a TensorRT ",(0,s.jsx)(n.code,{children:".engine"})," file:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"print('```')\r\nimport os\r\nimport torch\r\nfrom ultralytics import YOLO\r\n\r\n## Define model parameters\r\nmodelname = 'yolov5s6u'  # Model variant\r\nin_shape = (1, 3, 1280, 1280)  # Input shape (batch size, channels, height, width)\r\nfp16 = True  # Enable FP16 precision for optimization\r\n\r\n## Define engine file name\r\nenginename = f\"{modelname}.{in_shape}{'.FP16' if fp16 else ''}.engine\"\r\n\r\n## Create a dummy input tensor\r\ndummy_input = torch.rand(in_shape).cuda()\r\n\r\n## Load the YOLOv5 model\r\nmodel = YOLO(f\"{modelname}.pt\")\r\n\r\n## Export to TensorRT if the engine does not exist\r\nif not os.path.isfile(enginename):\r\n    model.export(format=\"engine\", batch=in_shape[0], half=fp16, imgsz=in_shape[-1])\r\n    os.rename(f\"{modelname}.engine\", enginename)\r\n\r\nprint(f\"Model exported to {enginename}\")\r\nprint('```')\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5s6u.pt to 'yolov5s6u.pt'...\r\n\r\n\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.6M/29.6M [00:02<00:00, 11.7MB/s]\r\n\r\nWARNING  TensorRT requires GPU export, automatically assigning device=0\r\nUltralytics 8.3.69  Python-3.11.5 torch-2.5.1+cu118 CUDA:0 (NVIDIA GeForce GTX 1650, 4096MiB)\r\n\r\n\r\n\r\n\r\n\r\nYOLOv5s6u summary (fused): 253 layers, 15,293,680 parameters, 0 gradients, 24.4 GFLOPs\r\n\r\n\x1b[34m\x1b[1mPyTorch:\x1b[0m starting from 'yolov5s6u.pt' with input shape (1, 3, 1280, 1280) BCHW and output shape(s) (1, 84, 34000) (29.6 MB)\r\n\r\n\x1b[34m\x1b[1mONNX:\x1b[0m starting export with onnx 1.15.0 opset 19...\r\n\x1b[34m\x1b[1mONNX:\x1b[0m slimming with onnxslim 0.1.48...\r\n\x1b[34m\x1b[1mONNX:\x1b[0m export success  3.4s, saved as 'yolov5s6u.onnx' (59.1 MB)\r\n\r\n\x1b[34m\x1b[1mTensorRT:\x1b[0m starting export with TensorRT 10.8.0.43...\r\n\x1b[34m\x1b[1mTensorRT:\x1b[0m input \"images\" with shape(1, 3, 1280, 1280) DataType.FLOAT\r\n\x1b[34m\x1b[1mTensorRT:\x1b[0m output \"output0\" with shape(1, 84, 34000) DataType.FLOAT\r\n\x1b[34m\x1b[1mTensorRT:\x1b[0m building FP16 engine as yolov5s6u.engine\r\n\x1b[34m\x1b[1mTensorRT:\x1b[0m export success  529.7s, saved as 'yolov5s6u.engine' (36.8 MB)\r\n\r\nExport complete (530.6s)\r\nResults saved to \x1b[1mD:\\github\\qinhy\\deep-dive-yolo\\docusaurus\\src\x1b[0m\r\nPredict:         yolo predict task=detect model=yolov5s6u.engine imgsz=1280 half \r\nValidate:        yolo val task=detect model=yolov5s6u.engine imgsz=1280 data=/usr/src/app/ultralytics/datasets/coco.yaml half \r\nVisualize:       https://netron.app\r\nModel exported to yolov5s6u.(1, 3, 1280, 1280).FP16.engine\n"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Loads the YOLOv5 model using ",(0,s.jsx)(n.code,{children:"ultralytics.YOLO"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Defines input shape and floating-point precision."}),"\n",(0,s.jsx)(n.li,{children:"Checks if a TensorRT engine exists; if not, it exports one."}),"\n",(0,s.jsxs)(n.li,{children:["Uses ",(0,s.jsx)(n.code,{children:'model.export(format="engine")'})," to convert PyTorch ",(0,s.jsx)(n.code,{children:".pt"})," to TensorRT."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"step-2-run-inference-with-tensorrt",children:"Step 2: Run Inference with TensorRT"}),"\n",(0,s.jsx)(n.p,{children:"Once the model is exported, we can use TensorRT for inference:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"print('```')\r\nimport matplotlib.pyplot as plt\r\nfrom ultralytics import YOLO\r\n\r\n## Define model parameters\r\nmodelname = 'yolov5s6u'  # Model variant\r\nin_shape = (1, 3, 1280, 1280)  # Input shape (batch size, channels, height, width)\r\nfp16 = True  # Enable FP16 precision for optimization\r\nenginename = f\"{modelname}.{in_shape}{'.FP16' if fp16 else ''}.engine\"\r\n\r\n## Load the optimized TensorRT model\r\ntensorrt_model = YOLO(enginename,task='detect')\r\n\r\n## Run inference on an image\r\nresults = tensorrt_model.predict('https://ultralytics.com/images/zidane.jpg', imgsz=in_shape[-1])\r\n\r\n## Print confidence scores of detected objects\r\nprint([r.boxes.conf for r in results])\r\n\r\nprint('```')\r\n\r\n# Convert results to OpenCV format for visualization\r\nannotated_img = results[0].plot()[:,:,::-1]  # This method overlays detections\r\n# Plot the image with detections\r\nplt.figure(figsize=(10, 8))\r\nplt.imshow(annotated_img)\r\nplt.axis(\"off\")\r\nplt.title(\"YOLO TensorRT Detection Results\")\r\nplt.show()\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Loading yolov5s6u.(1, 3, 1280, 1280).FP16.engine for TensorRT inference...\r\n\r\nFound https://ultralytics.com/images/zidane.jpg locally at zidane.jpg\r\nimage 1/1 D:\\github\\qinhy\\deep-dive-yolo\\docusaurus\\src\\zidane.jpg: 1280x1280 2 persons, 1 tie, 31.6ms\r\nSpeed: 17.9ms preprocess, 31.6ms inference, 1.9ms postprocess per image at shape (1, 3, 1280, 1280)\r\n[tensor([0.8862, 0.8535, 0.8008], device='cuda:0')]\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:r(4419).A+"",width:"794",height:"478"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Loads the TensorRT model."}),"\n",(0,s.jsx)(n.li,{children:"Sets the image size for inference."}),"\n",(0,s.jsx)(n.li,{children:"Runs object detection on an input image."}),"\n",(0,s.jsx)(n.li,{children:"Prints the confidence scores of detected objects."}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"step-3-benchmark-performance",children:"Step 3: Benchmark Performance"}),"\n",(0,s.jsx)(n.p,{children:"To compare performance between PyTorch and TensorRT, you can run a benchmark:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"print('```')\r\nimport time\r\nfrom ultralytics import YOLO\r\n\r\n## Define model parameters\r\nmodelname = 'yolov5s6u'  # Model variant\r\nin_shape = (1, 3, 1280, 1280)  # Input shape (batch size, channels, height, width)\r\nfp16 = True  # Enable FP16 precision for optimization\r\nenginename = f\"{modelname}.{in_shape}{'.FP16' if fp16 else ''}.engine\"\r\n\r\n## Load models\r\npytorch_model = YOLO(f\"{modelname}.pt\",task='detect')\r\ntensorrt_model = YOLO(enginename,task='detect')\r\n\r\n## Load test image\r\nimage_path = 'http://images.cocodataset.org/val2017/000000005477.jpg'\r\n\r\n## PyTorch Inference Time\r\nstart = time.time()\r\npytorch_results = pytorch_model.predict(image_path, imgsz=in_shape[-1])\r\nend = time.time()\r\nprint(f\"PyTorch Inference Time: {end - start:.4f} seconds\")\r\n\r\n## TensorRT Inference Time\r\nstart = time.time()\r\ntensorrt_results = tensorrt_model.predict(image_path, imgsz=in_shape[-1])\r\nend = time.time()\r\nprint(f\"TensorRT Inference Time: {end - start:.4f} seconds\")\r\n\r\n\r\n## Print confidence scores of detected objects\r\nprint([r.boxes.conf for r in results])\r\n\r\nprint('```')\r\n\r\n# Convert results to OpenCV format for visualization\r\nannotated_img = tensorrt_results[0].plot()[:,:,::-1]  # This method overlays detections\r\n# Plot the image with detections\r\nplt.figure(figsize=(10, 8))\r\nplt.imshow(annotated_img)\r\nplt.axis(\"off\")\r\nplt.title(\"YOLO Detection Results\")\r\nplt.show()\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"\r\nFound http://images.cocodataset.org/val2017/000000005477.jpg locally at 000000005477.jpg\r\nimage 1/1 D:\\github\\qinhy\\deep-dive-yolo\\docusaurus\\src\\000000005477.jpg: 704x1280 2 airplanes, 31.2ms\r\nSpeed: 6.0ms preprocess, 31.2ms inference, 0.0ms postprocess per image at shape (1, 3, 704, 1280)\r\nPyTorch Inference Time: 0.4736 seconds\r\nLoading yolov5s6u.(1, 3, 1280, 1280).FP16.engine for TensorRT inference...\r\n\r\nFound http://images.cocodataset.org/val2017/000000005477.jpg locally at 000000005477.jpg\r\nimage 1/1 D:\\github\\qinhy\\deep-dive-yolo\\docusaurus\\src\\000000005477.jpg: 1280x1280 2 airplanes, 16.4ms\r\nSpeed: 16.9ms preprocess, 16.4ms inference, 0.0ms postprocess per image at shape (1, 3, 1280, 1280)\r\nTensorRT Inference Time: 0.1616 seconds\r\n[tensor([0.8862, 0.8535, 0.8008], device='cuda:0')]\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{alt:"png",src:r(6869).A+"",width:"794",height:"464"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TensorRT should be significantly faster than PyTorch"}),", especially for batch inference."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"conclusion-1",children:"Conclusion"}),"\n",(0,s.jsx)(n.p,{children:"Optimizing YOLOv5 with TensorRT dramatically boosts inference speed while maintaining accuracy. This guide covered:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Exporting YOLOv5 to TensorRT."}),"\n",(0,s.jsx)(n.li,{children:"Running inference on an image."}),"\n",(0,s.jsx)(n.li,{children:"Benchmarking performance."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\ude80 ",(0,s.jsx)(n.strong,{children:"Start optimizing your YOLOv5 models with TensorRT today!"})," \ud83d\ude80"]}),"\n",(0,s.jsx)(n.h3,{id:"additional-to-do-debayer-with-tensorrt",children:"Additional To-Do: Debayer with TensorRT"}),"\n",(0,s.jsxs)(n.p,{children:["This document provides an overview of implementing a ",(0,s.jsx)(n.strong,{children:"Debayer5x5"})," module using PyTorch and converting it into an optimized ",(0,s.jsx)(n.strong,{children:"TensorRT"})," engine."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"overview",children:"Overview"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.strong,{children:"Debayer5x5"})," module performs ",(0,s.jsx)(n.strong,{children:"demosaicing"})," of Bayer images using the Malver-He-Cutler algorithm. The implementation follows OpenCV\u2019s Bayer pattern and provides better edge sharpness compared to other interpolation methods."]}),"\n",(0,s.jsx)(n.p,{children:"The following steps outline the process:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Build a PyTorch Model"})," (",(0,s.jsx)(n.code,{children:"Debayer5x5"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Export the Model to ONNX"})," for interoperability."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Convert the ONNX Model to TensorRT"})," for optimized inference."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import enum\r\nimport torch\r\nimport tensorrt as trt\r\n\r\n### Define the `Debayer5x5` PyTorch Model\r\n# The `Debayer5x5` model applies a **5x5 convolution filter** to interpolate missing color information from a Bayer pattern.\r\n\r\nclass Debayer5x5(torch.nn.Module):\r\n    # from https://github.com/cheind/pytorch-debayer\r\n    """Demosaicing of Bayer images using Malver-He-Cutler algorithm.\r\n\r\n    Requires BG-Bayer color filter array layout. That is,\r\n    the image[1,1]=\'B\', image[1,2]=\'G\'. This corresponds\r\n    to OpenCV naming conventions.\r\n\r\n    Compared to Debayer2x2 this method does not use upsampling.\r\n    Compared to Debayer3x3 the algorithm gives sharper edges and\r\n    less chromatic effects.\r\n\r\n    ## References\r\n    Malvar, Henrique S., Li-wei He, and Ross Cutler.\r\n    "High-quality linear interpolation for demosaicing of Bayer-patterned\r\n    color images." 2004\r\n    """\r\n    class Layout(enum.Enum):\r\n        """Possible Bayer color filter array layouts.\r\n\r\n        The value of each entry is the color index (R=0,G=1,B=2)\r\n        within a 2x2 Bayer block.\r\n        """\r\n        RGGB = (0, 1, 1, 2)\r\n        GRBG = (1, 0, 2, 1)\r\n        GBRG = (1, 2, 0, 1)\r\n        BGGR = (2, 1, 1, 0)\r\n\r\n    def __init__(self, layout: Layout = Layout.RGGB):\r\n        super(Debayer5x5, self).__init__()\r\n        self.layout = layout\r\n        # fmt: off\r\n        self.kernels = torch.nn.Parameter(\r\n            torch.tensor(\r\n                [\r\n                    # G at R,B locations\r\n                    # scaled by 16\r\n                    [ 0,  0, -2,  0,  0], # noqa\r\n                    [ 0,  0,  4,  0,  0], # noqa\r\n                    [-2,  4,  8,  4, -2], # noqa\r\n                    [ 0,  0,  4,  0,  0], # noqa\r\n                    [ 0,  0, -2,  0,  0], # noqa\r\n\r\n                    # R,B at G in R rows\r\n                    # scaled by 16\r\n                    [ 0,  0,  1,  0,  0], # noqa\r\n                    [ 0, -2,  0, -2,  0], # noqa\r\n                    [-2,  8, 10,  8, -2], # noqa\r\n                    [ 0, -2,  0, -2,  0], # noqa\r\n                    [ 0,  0,  1,  0,  0], # noqa\r\n\r\n                    # R,B at G in B rows\r\n                    # scaled by 16\r\n                    [ 0,  0, -2,  0,  0], # noqa\r\n                    [ 0, -2,  8, -2,  0], # noqa\r\n                    [ 1,  0, 10,  0,  1], # noqa\r\n                    [ 0, -2,  8, -2,  0], # noqa\r\n                    [ 0,  0, -2,  0,  0], # noqa\r\n\r\n                    # R at B and B at R\r\n                    # scaled by 16\r\n                    [ 0,  0, -3,  0,  0], # noqa\r\n                    [ 0,  4,  0,  4,  0], # noqa\r\n                    [-3,  0, 12,  0, -3], # noqa\r\n                    [ 0,  4,  0,  4,  0], # noqa\r\n                    [ 0,  0, -3,  0,  0], # noqa\r\n\r\n                    # R at R, B at B, G at G\r\n                    # identity kernel not shown\r\n                ]\r\n            ).view(4, 1, 5, 5).float() / 16.0,\r\n            requires_grad=False,\r\n        )\r\n        # fmt: on\r\n\r\n        self.index = torch.nn.Parameter(\r\n            # Below, note that index 4 corresponds to identity kernel\r\n            self._index_from_layout(layout),\r\n            requires_grad=False,\r\n        )\r\n\r\n    def forward(self, x):\r\n        """Debayer image.\r\n\r\n        Parameters\r\n        ----------\r\n        x : Bx1xHxW tensor\r\n            Images to debayer\r\n\r\n        Returns\r\n        -------\r\n        rgb : Bx3xHxW tensor\r\n            Color images in RGB channel order.\r\n        """\r\n        B, C, H, W = x.shape\r\n\r\n        xpad = torch.nn.functional.pad(x, (2, 2, 2, 2), mode="reflect")\r\n        planes = torch.nn.functional.conv2d(xpad, self.kernels, stride=1)\r\n        planes = torch.cat(\r\n            (planes, x), 1\r\n        )  # Concat with input to give identity kernel Bx5xHxW\r\n        rgb = torch.gather(\r\n            planes,\r\n            1,\r\n            self.index.repeat(\r\n                1,\r\n                1,\r\n                torch.div(H, 2, rounding_mode="floor"),\r\n                torch.div(W, 2, rounding_mode="floor"),\r\n            ).expand(\r\n                B, -1, -1, -1\r\n            ),  # expand for singleton batch dimension is faster\r\n        )\r\n        return torch.clamp(rgb, 0, 1)\r\n\r\n    def _index_from_layout(self, layout: Layout = Layout) -> torch.Tensor:\r\n        """Returns a 1x3x2x2 index tensor for each color RGB in a 2x2 bayer tile.\r\n\r\n        Note, the index corresponding to the identity kernel is 4, which will be\r\n        correct after concatenating the convolved output with the input image.\r\n        """\r\n        #       ...\r\n        # ... b g b g ...\r\n        # ... g R G r ...\r\n        # ... b G B g ...\r\n        # ... g r g r ...\r\n        #       ...\r\n        # fmt: off\r\n        rggb = torch.tensor(\r\n            [\r\n                # dest channel r\r\n                [4, 1],  # pixel is R,G1\r\n                [2, 3],  # pixel is G2,B\r\n                # dest channel g\r\n                [0, 4],  # pixel is R,G1\r\n                [4, 0],  # pixel is G2,B\r\n                # dest channel b\r\n                [3, 2],  # pixel is R,G1\r\n                [1, 4],  # pixel is G2,B\r\n            ]\r\n        ).view(1, 3, 2, 2)\r\n        # fmt: on\r\n        return {\r\n            layout.RGGB: rggb,\r\n            layout.GRBG: torch.roll(rggb, 1, -1),\r\n            layout.GBRG: torch.roll(rggb, 1, -2),\r\n            layout.BGGR: torch.roll(rggb, (1, 1), (-1, -2)),\r\n        }.get(layout)\r\n\r\n\r\n#### Key Features:\r\n# - Implements **Malvar-He-Cutler** algorithm for Bayer interpolation.\r\n# - Supports **different Bayer layouts** (`RGGB`, `GRBG`, `GBRG`, `BGGR`).\r\n# - Uses **fixed convolution kernels** for demosaicing.\r\n\r\n### Build the PyTorch Model\r\n# We instantiate the `Debayer5x5` model and create a dummy input tensor.\r\n\r\ndef build_torch_model(shape):\r\n    dummy_input = torch.rand(shape).cuda()\r\n    model = Debayer5x5()\r\n    model.eval().cuda()\r\n\r\n    print(model(dummy_input).shape)\r\n    torch_file_path = f\'{modelname}.{shape}.pt\'\r\n    torch.save(model.state_dict(), torch_file_path)\r\n    \r\n    return model, dummy_input\r\n\r\n### 4. Export Model to ONNX\r\n# Convert the trained PyTorch model to an **ONNX** format for further conversion.\r\n\r\ndef export_torch_onnx_model(model, x, onnx_model_path):\r\n    torch.onnx.export(\r\n        model,\r\n        x,\r\n        onnx_model_path,\r\n        opset_version=11,\r\n        input_names=["input"],\r\n        output_names=["output"]\r\n    )\r\n    print(f"ONNX model saved to {onnx_model_path}")\r\n\r\n### 5. Convert ONNX Model to TensorRT Engine\r\n# Use TensorRT to build an optimized inference engine.\r\n\r\nTRT_LOGGER = trt.Logger(trt.Logger.WARNING)\r\n\r\ndef build_static_engine(onnx_file_path, engine_file_path, fp16=True):\r\n    with trt.Builder(TRT_LOGGER) as builder, \\\r\n         builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)) as network, \\\r\n         trt.OnnxParser(network, TRT_LOGGER) as parser, \\\r\n         builder.create_builder_config() as config:\r\n        \r\n        if fp16 and builder.platform_has_fast_fp16:\r\n            print("Platform supports FP16, enabling FP16 optimization...")\r\n            config.set_flag(trt.BuilderFlag.FP16)\r\n            engine_file_path = engine_file_path.replace(\'.trt\', \'.FP16.trt\')\r\n\r\n        with open(onnx_file_path, \'rb\') as model:\r\n            if not parser.parse(model.read()):\r\n                print("Failed to parse the ONNX file.")\r\n                for error in range(parser.num_errors):\r\n                    print(parser.get_error(error))\r\n                return None\r\n\r\n        engine = builder.build_serialized_network(network, builder.create_builder_config())\r\n        if engine is None:\r\n            print("Failed to build the engine.")\r\n            return None\r\n\r\n        with open(engine_file_path, "wb") as f:\r\n            f.write(engine)\r\n        print(f"TensorRT engine saved to {engine_file_path}")\r\n\r\n## Full Execution Pipeline\r\nprint(\'```\')\r\nmodelname = "debayer5x5"\r\nshape = (1,1,1280,1280)\r\n\r\ntorch_file_path = f\'{modelname}.{shape}.pt\'\r\nonnx_file_path = f\'{modelname}.{shape}.onnx\'\r\nengine_file_path = f\'{modelname}.{shape}.trt\'\r\n\r\nmodel, x = build_torch_model(shape)\r\nexport_torch_onnx_model(model, x, onnx_file_path)\r\nbuild_static_engine(onnx_file_path, engine_file_path)\r\nprint(\'```\')\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"torch.Size([1, 3, 1280, 1280])\r\nONNX model saved to debayer5x5.(1, 1, 1280, 1280).onnx\r\nPlatform supports FP16, enabling FP16 optimization...\r\nTensorRT engine saved to debayer5x5.(1, 1, 1280, 1280).FP16.trt\n"})}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Define the PyTorch Model"})," (",(0,s.jsx)(n.code,{children:"Debayer5x5"}),")."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Build & Save the PyTorch Model"})," (",(0,s.jsx)(n.code,{children:".pt"})," file)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Convert to ONNX"})," (",(0,s.jsx)(n.code,{children:".onnx"})," file)."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimize with TensorRT"})," (",(0,s.jsx)(n.code,{children:".trt"})," file)."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"handling-large-images",children:(0,s.jsx)(n.strong,{children:"Handling Large Images"})}),"\n",(0,s.jsx)(n.p,{children:"Processing large images efficiently is a crucial challenge in deep learning, especially when working with object detection or segmentation models. A common approach is to divide large images into smaller overlapping or non-overlapping patches, process them separately, and then merge the results back into the original image space."}),"\n",(0,s.jsxs)(n.p,{children:["This section discusses a ",(0,s.jsx)(n.strong,{children:"Sliding Window Approach"})," for handling large images using Python's NumPy and PyTorch."]}),"\n",(0,s.jsx)(n.p,{children:"The resizing of a large image is also an acceptable solution, but we do not discuss it here."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"apply-sliding-window",children:(0,s.jsx)(n.strong,{children:"Apply Sliding Window"})}),"\n",(0,s.jsxs)(n.p,{children:["A ",(0,s.jsx)(n.strong,{children:"sliding window"})," technique helps break down large images into smaller, manageable patches that can be processed independently. Below is an implementation using NumPy."]}),"\n",(0,s.jsx)(n.h4,{id:"implementation-sliding-window-extraction",children:(0,s.jsx)(n.strong,{children:"Implementation: Sliding Window Extraction"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nimport torch\r\n\r\nclass SlidingWindowProcessor:\r\n    def __init__(self, window_size=(512, 512), stride=(256, 256)):\r\n        self.window_size = window_size\r\n        self.stride = stride\r\n\r\n    def apply_sliding_window(self, data):\r\n        """\r\n        Extracts sliding windows from an image.\r\n        :param data: NumPy array representing the image (H, W, C).\r\n        :return: List of image patches and their corresponding bounding boxes.\r\n        """\r\n        H, W, C = data.shape\r\n        wH, wW = self.window_size\r\n        sH, sW = self.stride\r\n\r\n        if wH > H or wW > W:\r\n            raise ValueError(f"Window size ({wH}, {wW}) must be <= image size ({H}, {W}).")\r\n\r\n        windows_list = []\r\n        offsets_xyxy = []\r\n\r\n        for row_start in range(0, H - wH + 1, sH):\r\n            for col_start in range(0, W - wW + 1, sW):\r\n                window = data[row_start: row_start + wH, col_start: col_start + wW, :]\r\n                windows_list.append(window)\r\n                offsets_xyxy.append((col_start, row_start, col_start + wW, row_start + wH))  # Bounding box\r\n\r\n        return windows_list, offsets_xyxy\r\n\r\n# Example Usage\r\nimage = np.random.randint(0, 255, (1024, 1024, 3), dtype=np.uint8)  # Simulated large image\r\nprocessor = SlidingWindowProcessor(window_size=(512, 512), stride=(256, 256))\r\nwindows, bboxes = processor.apply_sliding_window(image)\r\n\r\nprint(f"Extracted {len(windows)} windows with bounding boxes: {bboxes[:5]}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udd39 ",(0,s.jsx)(n.strong,{children:"Explanation:"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The image is divided into ",(0,s.jsx)(n.strong,{children:"512\xd7512 patches"})," with a stride of ",(0,s.jsx)(n.strong,{children:"256 pixels"}),"."]}),"\n",(0,s.jsx)(n.li,{children:"Overlapping patches help preserve details at the borders."}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"offsets_xyxy"})," keeps track of the ",(0,s.jsx)(n.strong,{children:"bounding box coordinates"})," for each extracted patch."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"merge-sliding-window-and-yolov5-results",children:(0,s.jsx)(n.strong,{children:"Merge Sliding Window and YOLOv5 results"})}),"\n",(0,s.jsxs)(n.p,{children:["Once the sliding window approach is applied, each patch can be passed into ",(0,s.jsx)(n.strong,{children:"YOLOv5"})," for object detection. The detected bounding boxes need to be adjusted back to the original image coordinates."]}),"\n",(0,s.jsx)(n.h4,{id:"applying-yolov5-to-patches",children:(0,s.jsx)(n.strong,{children:"Applying YOLOv5 to Patches"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'from ultralytics import YOLO\r\n\r\n# Load YOLOv5 model\r\nmodel = YOLO("yolov5s.pt")  # Load pretrained model\r\n\r\n# Process each window with YOLOv5\r\nall_detections = []\r\nfor i, (patch, bbox) in enumerate(zip(windows, bboxes)):\r\n    patch_tensor = torch.from_numpy(patch).permute(2, 0, 1).float().unsqueeze(0)  # Convert to tensor\r\n    results = model(patch_tensor)  # Run YOLOv5 on the patch\r\n    \r\n    for result in results.xyxy[0]:  # Extract detections\r\n        x_min, y_min, x_max, y_max, conf, cls = result.tolist()\r\n        \r\n        # Adjust coordinates to original image\r\n        x_min += bbox[0]\r\n        y_min += bbox[1]\r\n        x_max += bbox[0]\r\n        y_max += bbox[1]\r\n\r\n        all_detections.append((x_min, y_min, x_max, y_max, conf, cls))\r\n\r\nprint(f"Total detections after merging: {len(all_detections)}")\n'})}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udd39 ",(0,s.jsx)(n.strong,{children:"Explanation:"})]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:["Each patch is passed through ",(0,s.jsx)(n.strong,{children:"YOLOv5"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["Detected bounding boxes are converted back to ",(0,s.jsx)(n.strong,{children:"original image coordinates"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["The final detections are stored in ",(0,s.jsx)(n.code,{children:"all_detections"}),"."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"conclusion-2",children:(0,s.jsx)(n.strong,{children:"Conclusion"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["The ",(0,s.jsx)(n.strong,{children:"Sliding Window Approach"})," allows processing large images efficiently in smaller chunks."]}),"\n",(0,s.jsxs)(n.li,{children:["YOLOv5 can be applied to each patch, and detected objects are ",(0,s.jsx)(n.strong,{children:"mapped back"})," to the original image space."]}),"\n",(0,s.jsx)(n.li,{children:"This method ensures that large images do not exceed GPU memory constraints while maintaining detection accuracy."}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udca1 ",(0,s.jsx)(n.strong,{children:"Next Steps:"})]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Apply ",(0,s.jsx)(n.strong,{children:"Non-Maximum Suppression (NMS)"})," to merge overlapping detections."]}),"\n",(0,s.jsxs)(n.li,{children:["Use ",(0,s.jsx)(n.strong,{children:"post-processing techniques"})," to stitch segmentations if applied to segmentation models."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"multi-camera--large-scale-image-processing",children:"Multi-Camera & Large-Scale Image Processing"}),"\n",(0,s.jsx)(n.h2,{id:"deploying-on-edge-devices-raspberry-pi-jetson-nano",children:"Deploying on Edge Devices (Raspberry Pi, Jetson Nano)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python"})})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},4419:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/yolov5_10_1-81f6f4d6b7238bbe38fe642aef1acdd7.png"},6869:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/yolov5_12_1-8cc0bd7fe57adbb21a369da4ad11fc13.png"},4694:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/yolov5_4_1-ea9f104517f42c8736d76b328c983e3a.png"},8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>l});var i=r(6540);const s={},t=i.createContext(s);function o(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);